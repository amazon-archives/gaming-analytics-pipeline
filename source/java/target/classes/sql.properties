######################################################################################################################
# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved. *
# 
# Licensed under the Amazon Software License (the "License"). You may not use this file except in compliance *
# with the License. A copy of the License is located at *
# 
# http://aws.amazon.com/asl/ *
# 
# or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES *
# OR CONDITIONS OF ANY KIND, express or implied. See the License for the specific language governing permissions *
# and limitations under the License. *
######################################################################################################################

#
# For more details, see com.amazonaws.gaming.analytics.common.AppConfiguration
#
# Order of precendence:
#
# 1. test.<project>.<connector_type>.<property_name>
# 2. test.common.<connector_type>.<property_name>
# 3. <project>.<connector_type>.<property_name>
# 4. common.<connector_type>.<property_name>
# 5. <project>.<property_name>
# 6. common.<property_name>
# 7. Java System Property (System.getProperty)
#    - Based on the way we set up configuration, system properties are reference-able in config files
#
# NOTE: "test" is auto-prepended to property names when the app is run
#       locally (e.g. in Eclipse or via command line)

analytics.jsonpath_filename = analytics-jsonpath.json
analytics.jdbc_driver_name = com.amazon.redshift.jdbc42.Driver
analytics.load_staging_table = analytics_staging_load
analytics.dedupe_staging_table_prefix = analytics_staging_dedupe
analytics.events_table_prefix_no_schema = events
analytics.events_table_prefix = ${redshift_schema}.${analytics.events_table_prefix_no_schema}
analytics.events_view_name = ${redshift_schema}.events
analytics.jsonpath_s3_path = s3://${s3_config_bucket}/${analytics.jsonpath_filename}

#Field sizes for database columns (also specified in telemetry schema)
analytics.event.app_name_max_length = 64
analytics.event.app_version_max_length = 64
analytics.event.event_version_max_length = 64
analytics.event.event_id_max_length = 36
analytics.event.event_type_max_length = 256
analytics.event.client_id_max_length = 36
analytics.event.level_id_max_length = 64

#Create the staging table
#The temp table should have a field for every single field in the input JSON event structure.
#(Make sure the order of fields here matches the ordering in the analytics-jsonpath.json file)
analytics.sql.create_temp_table = CREATE TEMP TABLE %s \
( \
app_name VARCHAR(${analytics.event.app_name_max_length}) ENCODE ZSTD\, \
app_version VARCHAR(${analytics.event.app_version_max_length}) ENCODE ZSTD\, \
event_version VARCHAR(${analytics.event.event_version_max_length}) ENCODE ZSTD\, \
event_id VARCHAR(${analytics.event.event_id_max_length}) NOT NULL ENCODE ZSTD\, \
event_type VARCHAR(${analytics.event.event_type_max_length}) NOT NULL ENCODE ZSTD\, \
event_timestamp TIMESTAMP NOT NULL ENCODE RAW\, \
server_timestamp TIMESTAMP NOT NULL ENCODE RAW\, \
client_id VARCHAR(${analytics.event.client_id_max_length}) ENCODE ZSTD\, \
level_id VARCHAR(${analytics.event.level_id_max_length}) ENCODE ZSTD\, \
position_x FLOAT ENCODE RAW\, \
position_y FLOAT ENCODE RAW\, \
PRIMARY KEY(event_timestamp\, event_id) \
) \
DISTKEY(client_id) \
COMPOUND SORTKEY(event_timestamp\, event_type);

#Create a single main event table
analytics.sql.create_event_table = CREATE TABLE IF NOT EXISTS %s \
( \
app_name VARCHAR(${analytics.event.app_name_max_length}) ENCODE ZSTD\, \
app_version VARCHAR(${analytics.event.app_version_max_length}) ENCODE ZSTD\, \
event_version VARCHAR(${analytics.event.event_version_max_length}) ENCODE ZSTD\, \
event_id VARCHAR(${analytics.event.event_id_max_length}) NOT NULL ENCODE ZSTD\, \
event_type VARCHAR(${analytics.event.event_type_max_length}) NOT NULL ENCODE ZSTD\, \
event_timestamp TIMESTAMP NOT NULL ENCODE RAW\, \
server_timestamp TIMESTAMP NOT NULL ENCODE RAW\, \
client_id VARCHAR(${analytics.event.client_id_max_length}) ENCODE ZSTD\, \
level_id VARCHAR(${analytics.event.level_id_max_length}) ENCODE ZSTD\, \
position_x FLOAT ENCODE RAW\, \
position_y FLOAT ENCODE RAW\, \
PRIMARY KEY(event_timestamp\, event_id) \
) \
DISTKEY(client_id) \
COMPOUND SORTKEY(event_timestamp\, event_type); \
GRANT ALL PRIVILEGES ON %s TO ${redshift_worker_username}; \
ALTER TABLE %s OWNER TO ${redshift_worker_username}; \
GRANT SELECT ON %s TO ${redshift_readonly_username};

#SQL to copy from the load_staging table to the dedupe staging table and remove duplicates
analytics.sql.event_dedupe_insert = INSERT INTO %s \
(event_timestamp\, \
server_timestamp\, \
event_version\, \
event_id\, \
event_type\, \
client_id\, \
app_name\, \
app_version\, \
level_id\, \
position_x\, \
position_y) \
(SELECT intermediate.event_timestamp\, \
intermediate.server_timestamp\, \
intermediate.event_version\, \
intermediate.event_id\, \
intermediate.event_type\, \
intermediate.client_id\, \
intermediate.app_name\, \
intermediate.app_version\, \
intermediate.level_id\, \
intermediate.position_x\, \
intermediate.position_y \
FROM \
(SELECT DISTINCT load_staging.* \
FROM ${analytics.load_staging_table} load_staging LEFT JOIN %s events \
ON load_staging.event_timestamp=events.event_timestamp AND load_staging.event_id=events.event_id \
WHERE DATE_PART('year'\, load_staging.event_timestamp)=%d \
AND DATE_PART('month'\, load_staging.event_timestamp)=%d \
AND events.event_id IS NULL \
ORDER BY load_staging.event_timestamp\, load_staging.event_type \
) as intermediate);

#SQL to copy unique events from the dedupe staging table to the final main table
analytics.sql.event_final_insert = INSERT INTO %s \
(event_timestamp\, \
server_timestamp\, \
event_version\, \
event_id\, \
event_type\, \
client_id\, \
app_name\, \
app_version\, \
level_id\, \
position_x\, \
position_y) \
(SELECT dedupe_staging.event_timestamp\, \
dedupe_staging.server_timestamp\, \
dedupe_staging.event_version\, \
dedupe_staging.event_id\, \
dedupe_staging.event_type\, \
dedupe_staging.client_id\, \
dedupe_staging.app_name\, \
dedupe_staging.app_version\, \
dedupe_staging.level_id\, \
dedupe_staging.position_x\, \
dedupe_staging.position_y \
FROM %s dedupe_staging \
ORDER BY dedupe_staging.event_timestamp\, dedupe_staging.event_type);

#COPY data from S3 to Redshift staging table
#NOTE: Don't put a semicolon on the end of this because it's a prefix, not a full SQL statement
analytics.sql.s3_copy_prefix = COPY ${analytics.load_staging_table} \
FROM 's3://${s3_telemetry_bucket}/%s' \
EMPTYASNULL \
BLANKSASNULL \
GZIP \
TIMEFORMAT AS 'epochmillisecs' \
MAXERROR AS 0 \
MANIFEST \
JSON '${analytics.jsonpath_s3_path}'

#Queries to create a UNION ALL view across event tables
#NOTE: Don't put a semicolon on the end of this because it's a prefix, not a full SQL statement
analytics.sql.create_view_prefix = CREATE OR REPLACE VIEW ${analytics.events_view_name} AS
analytics.sql.create_view_suffix = GRANT ALL PRIVILEGES ON ${analytics.events_view_name} TO ${redshift_worker_username}; \
ALTER TABLE ${analytics.events_view_name} OWNER TO ${redshift_worker_username}; \
GRANT SELECT ON ${analytics.events_view_name} TO ${redshift_readonly_username};

#Find all time-esries event tables
analytics.sql.get_all_tables = SELECT DISTINCT TRIM(relname) \
FROM stv_tbl_perm \
join pg_class on pg_class.oid = stv_tbl_perm.id \
join pg_namespace on pg_namespace.oid = relnamespace \
WHERE nspname='${redshift_schema}' \
AND relname LIKE '${analytics.events_table_prefix_no_schema}%%' \
ORDER BY relname;

#Analyze a table
analytics.sql.analyze_table = ANALYZE %s;

#Vacuum a table
analytics.sql.vacuum_table = VACUUM %s;

#Vacuum and reindex a table
analytics.sql.vacuum_reindex_table = VACUUM REINDEX %s;

#Drop a table
analytics.sql.drop_table = DROP TABLE IF EXISTS %s CASCADE;

#Get the number of records that were copied in the previous COPY command
analytics.sql.get_copy_count = SELECT pg_last_copy_count();

#This will return the number of errors that occurred during the most recent load in the same session
analytics.sql.get_last_load_error_count = SELECT COUNT(*) \
FROM stl_load_errors \
WHERE query=PG_LAST_COPY_ID();

#Count the number of rows inserted by the previous query in this session
analytics.sql.get_insert_count = SELECT SUM(rows) \
FROM stl_insert \
WHERE query=pg_last_query_id();

#Deduping and ordering have intentionally been made part of the SQL, do not remove the DISTINCT or ORDER BY
#unless you plan on changing the code that executes this query too
analytics.sql.get_unique_years_months = SELECT DISTINCT DATE_PART(year\, event_timestamp) as year\, \
DATE_PART(month\,event_timestamp) as month \
FROM %s \
ORDER BY year ASC\, month ASC;
